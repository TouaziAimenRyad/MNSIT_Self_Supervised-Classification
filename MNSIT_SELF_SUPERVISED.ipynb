{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Syd2eX9Q2m"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VplwwW749Q2s"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import gzip\n",
        "from urllib import request\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qXz8jFZ9Q2u"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1M_6dqfr9Q2u"
      },
      "outputs": [],
      "source": [
        "def load_mnist(pickle_path):\n",
        "    with open(pickle_path, 'rb') as f:\n",
        "        x_train, x_test, y_train, y_test = pickle.load(f).values()\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def load_mnist_semisupervised(pickle_path):\n",
        "    x_train, x_test, y_train, y_test = load_mnist(pickle_path)\n",
        "\n",
        "    labels = list(set(y_train))\n",
        "    print(f\"Labels: {labels}\")\n",
        "\n",
        "    data_dict = {}\n",
        "\n",
        "    data_dict[\"x_train_labeled\"], data_dict[\"x_train_unlabeled\"], data_dict[\"y_train_labeled\"], data_dict[\"y_train_unlabeled\"] = train_test_split(x_train, y_train, train_size=100, random_state=42, stratify=y_train)\n",
        "\n",
        "    # to verify number of samples for each class\n",
        "    classes, counts = np.unique(data_dict[\"y_train_labeled\"], return_counts=True)\n",
        "\n",
        "    data_dict[\"x_train\"] = x_train\n",
        "    data_dict[\"y_train\"] = y_train\n",
        "    data_dict[\"x_test\"] = x_test\n",
        "    data_dict[\"y_test\"] = y_test\n",
        "\n",
        "    print(f\"Labeled Train Shape   : {data_dict['x_train_labeled'].shape}\")\n",
        "    print(f\"UnLabeled Train Shape : {data_dict['x_train_unlabeled'].shape}\")\n",
        "\n",
        "    return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voRds19E9Q2v",
        "outputId": "6ef2d592-834a-4698-a87a-8e321490cd03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ./data/tmp/train-images-idx3-ubyte.gz...\n",
            "Downloading ./data/tmp/t10k-images-idx3-ubyte.gz...\n",
            "Downloading ./data/tmp/train-labels-idx1-ubyte.gz...\n",
            "Downloading ./data/tmp/t10k-labels-idx1-ubyte.gz...\n",
            "Download Completed!\n",
            "Saving as pickle complete: ./data/mnist.pkl\n",
            "Download directory (./tmp) is cleaned and removed.\n"
          ]
        }
      ],
      "source": [
        "DATASET_DIR = \"./data/\"\n",
        "DOWNLOAD_DIR = \"./data/tmp/\"\n",
        "\n",
        "BASE_URL = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "\n",
        "TARGET_LIST = [\n",
        "    (\"train_images\", \"train-images-idx3-ubyte.gz\"),\n",
        "    (\"test_images\", \"t10k-images-idx3-ubyte.gz\"),\n",
        "    (\"train_labels\", \"train-labels-idx1-ubyte.gz\"),\n",
        "    (\"test_labels\", \"t10k-labels-idx1-ubyte.gz\")\n",
        "]\n",
        "\n",
        "\n",
        "def download():\n",
        "    if not os.path.exists(DOWNLOAD_DIR):\n",
        "        os.makedirs(DOWNLOAD_DIR)\n",
        "\n",
        "    for _, file_name in TARGET_LIST:\n",
        "        print(f\"Downloading {DOWNLOAD_DIR + file_name}...\")\n",
        "        request.urlretrieve(BASE_URL + file_name, DOWNLOAD_DIR + file_name)\n",
        "    print(\"Download Completed!\")\n",
        "\n",
        "\n",
        "def save_as_picke():\n",
        "    mnist = {}\n",
        "\n",
        "    for file_tag, file_name in TARGET_LIST[:2]:\n",
        "        with gzip.open(DOWNLOAD_DIR + file_name, 'rb') as f:\n",
        "            mnist[file_tag] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)/255\n",
        "\n",
        "    for file_tag, file_name in TARGET_LIST[-2:]:\n",
        "        with gzip.open(DOWNLOAD_DIR + file_name, 'rb') as f:\n",
        "            mnist[file_tag] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "    if not os.path.exists(DATASET_DIR):\n",
        "        os.makedirs(DATASET_DIR)\n",
        "\n",
        "    with open(DATASET_DIR + \"mnist.pkl\", 'wb') as f:\n",
        "        pickle.dump(mnist,f)\n",
        "    print(f\"Saving as pickle complete: {DATASET_DIR + 'mnist.pkl'}\")\n",
        "\n",
        "def clean():\n",
        "    if os.path.exists(DOWNLOAD_DIR):\n",
        "        for _, file_name in TARGET_LIST:\n",
        "            if os.path.exists(DOWNLOAD_DIR + file_name):\n",
        "                os.remove(DOWNLOAD_DIR + file_name)\n",
        "        os.rmdir(DOWNLOAD_DIR)\n",
        "        print(\"Download directory (./tmp) is cleaned and removed.\")\n",
        "\n",
        "download()\n",
        "save_as_picke()\n",
        "clean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Zk2d1w9Q2w",
        "outputId": "b1f5c38c-002d-414f-fc6f-fedd59d5c087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Labeled Train Shape   : (100, 784)\n",
            "UnLabeled Train Shape : (59900, 784)\n"
          ]
        }
      ],
      "source": [
        "mnist_path = \"./data/mnist.pkl\"\n",
        "\n",
        "data_dict = load_mnist_semisupervised(mnist_path)\n",
        "\n",
        "x_train_labeled = data_dict[\"x_train_labeled\"]\n",
        "x_train_unlabeled = data_dict[\"x_train_unlabeled\"]\n",
        "y_train_labeled = data_dict[\"y_train_labeled\"]\n",
        "y_train_unlabeled = data_dict[\"y_train_unlabeled\"]\n",
        "\n",
        "x_train = data_dict[\"x_train\"]\n",
        "y_train = data_dict[\"y_train\"]\n",
        "x_test = data_dict[\"x_test\"]\n",
        "y_test = data_dict[\"y_test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og_gWU259Q2w"
      },
      "source": [
        "# Create Custom Dataset Structure for Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b3cNBIZQ9Q2x"
      },
      "outputs": [],
      "source": [
        "class MNIST_Dataset(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        y = self.target[index]\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT5ZI_sd9Q2y"
      },
      "source": [
        "# Define Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Pg7P_E349Q2y"
      },
      "outputs": [],
      "source": [
        "# labeled + unlabeled training = 60000\n",
        "pretrain_num_epochs = 5\n",
        "pretrain_batch_size = 128\n",
        "all_train_dataset = TensorDataset(torch.Tensor(x_train),\n",
        "                                  torch.zeros(x_train.shape[0]))\n",
        "all_train_dataloader = DataLoader(all_train_dataset,\n",
        "                                     batch_size=pretrain_batch_size,\n",
        "                                     shuffle=True,\n",
        "                                     pin_memory=True)\n",
        "#-------------------------\n",
        "# unlabeled training = 59900\n",
        "unlabeled_train_dataset = TensorDataset(torch.Tensor(x_train_unlabeled),\n",
        "                                  torch.zeros(x_train_unlabeled.shape[0]))\n",
        "unlabeled_train_dataloader = DataLoader(unlabeled_train_dataset,\n",
        "                                     batch_size=pretrain_batch_size,\n",
        "                                     shuffle=True,\n",
        "                                     pin_memory=True)\n",
        "#-------------------------\n",
        "# labeled training = 100\n",
        "train_num_epochs = 10\n",
        "train_batch_size = 100\n",
        "train_dataset = TensorDataset(torch.Tensor(x_train),\n",
        "                              torch.LongTensor(y_train))\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                                     batch_size=train_batch_size,\n",
        "                                     shuffle=False,\n",
        "                                     pin_memory=True)\n",
        "#-------------------------\n",
        "# test = 10000\n",
        "test_batch_size = 128\n",
        "test_dataset = TensorDataset(torch.Tensor(x_test),\n",
        "                              torch.LongTensor(y_test))\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                                     batch_size=test_batch_size,\n",
        "                                     shuffle=False,\n",
        "                                     pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAxjSBbA9Q2x"
      },
      "source": [
        "# Define Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yh1Ne2Dt9Q2y"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 3)  # Compressed representation\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(3, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 28*28),\n",
        "            nn.Sigmoid()  # For normalized inputs\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl7iB65J9Q2y"
      },
      "source": [
        "# Unsupervised Pretraining of Autoencoder with all Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3hH1-xm9Q2z",
        "outputId": "c166dad3-0a86-466c-daa9-5e13baa6551e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4TazJIkk9Q2z"
      },
      "outputs": [],
      "source": [
        "autoencoder = Autoencoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIKianSR9Q2z",
        "outputId": "d6b5b115-f37c-4f0e-a9f1-bc1b8779c509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1/5, loss= 0.000241395, 5.58s\n",
            "epoch: 2/5, loss= 0.000195024, 2.43s\n",
            "epoch: 3/5, loss= 0.000169197, 2.42s\n",
            "epoch: 4/5, loss= 0.000177945, 3.14s\n",
            "epoch: 5/5, loss= 0.000157331, 2.59s\n"
          ]
        }
      ],
      "source": [
        "# Pretraining loop\n",
        "autoencoder.train()\n",
        "for epoch in range(pretrain_num_epochs):\n",
        "    loss = 0.0\n",
        "    t0 = time()\n",
        "    for data, _ in all_train_dataloader:\n",
        "        inputs = data.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = autoencoder(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss += loss.item()\n",
        "\n",
        "    loss = loss / len(all_train_dataloader)\n",
        "    t1 = time()\n",
        "    print(f\"epoch: {epoch+1}/{pretrain_num_epochs}, loss={loss: .6}, {t1-t0:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfm8R8v9Q2z"
      },
      "source": [
        "# Supervised Training using 100 labeled samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wd2Ic0PF9Q2z"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, pretrained_encoder):\n",
        "        super(MLP, self).__init__()\n",
        "        self.encoder = pretrained_encoder\n",
        "        # Add a classifier on top of the encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(3, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)  # Assuming 10 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cxXUDkst9Q2z"
      },
      "outputs": [],
      "source": [
        "# Freeze the encoder part during supervised training\n",
        "for param in autoencoder.encoder.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eEVX7UOl9Q20"
      },
      "outputs": [],
      "source": [
        "supervised_model = MLP(autoencoder.encoder).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(supervised_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RW8UFJUS9Q20"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = (correct_predictions / total_samples) * 100\n",
        "\n",
        "    print(f\"training loss: {avg_loss: .2f}, training accuracy: {accuracy: .2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w0iNOUkb9Q20"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients during evaluation\n",
        "        for data in data_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = (correct_predictions / total_samples) * 100\n",
        "\n",
        "    print(f\"evaluation loss: {avg_loss: .2f}, evaluation accuracy: {accuracy: .2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhkSEi6V9Q20",
        "outputId": "4ca52853-f249-46a0-8adf-5a58d1e3d26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "training loss:  1.11, training accuracy:  63.12\n",
            "Epoch 1\n",
            "training loss:  0.77, training accuracy:  74.27\n",
            "Epoch 2\n",
            "training loss:  0.73, training accuracy:  75.32\n",
            "Epoch 3\n",
            "training loss:  0.71, training accuracy:  75.67\n",
            "Epoch 4\n",
            "training loss:  0.70, training accuracy:  75.98\n",
            "Epoch 5\n",
            "training loss:  0.69, training accuracy:  76.16\n",
            "Epoch 6\n",
            "training loss:  0.69, training accuracy:  76.30\n",
            "Epoch 7\n",
            "training loss:  0.68, training accuracy:  76.37\n",
            "Epoch 8\n",
            "training loss:  0.68, training accuracy:  76.43\n",
            "Epoch 9\n",
            "training loss:  0.68, training accuracy:  76.49\n",
            "evaluation loss:  0.68, evaluation accuracy:  76.34\n"
          ]
        }
      ],
      "source": [
        "for i in range(train_num_epochs):\n",
        "    print(\"Epoch\", i)\n",
        "    train_model(supervised_model, train_dataloader, criterion, optimizer, device)\n",
        "\n",
        "evaluate_model(supervised_model, test_dataloader, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRM0uskv9Q20"
      },
      "source": [
        "# Labeling unlabeled Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "meY9ZVeo9Q20"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "labels_list = []\n",
        "\n",
        "supervised_model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # No need to track gradients during prediction\n",
        "    for data in unlabeled_train_dataloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # forward + prediction\n",
        "        outputs = supervised_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        data_list.append(inputs.cpu().detach())\n",
        "        labels_list.append(predicted.cpu().detach())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ewby4sD09Q20"
      },
      "outputs": [],
      "source": [
        "x_unlabeled = torch.vstack(data_list)\n",
        "y_unlabeled = torch.hstack(labels_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raoNdKiE9Q21"
      },
      "source": [
        "# Combining new labeled Data with train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOWVRiAZ9Q21",
        "outputId": "14b1edd5-6d62-4c07-8ae0-91d247fc97b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-b6e9635f48e5>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(torch.hstack((torch.Tensor(y_train_labeled), y_unlabeled)), dtype=torch.long))\n"
          ]
        }
      ],
      "source": [
        "# labeled + unlabeled training = 60000\n",
        "combined_train_num_epochs = 5\n",
        "combined_batch_size = 128\n",
        "combined_dataset = TensorDataset(torch.vstack((torch.Tensor(x_train_labeled), x_unlabeled)),\n",
        "                                  torch.tensor(torch.hstack((torch.Tensor(y_train_labeled), y_unlabeled)), dtype=torch.long))\n",
        "combined_dataloader = DataLoader(combined_dataset,\n",
        "                                     batch_size=combined_batch_size,\n",
        "                                     shuffle=True,\n",
        "                                     pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtoCf5yD9Q21",
        "outputId": "701eb923-ed1a-4486-f7e6-2fce3759b732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "training loss:  0.23, training accuracy:  96.50\n",
            "Epoch 1\n",
            "training loss:  0.18, training accuracy:  96.53\n",
            "Epoch 2\n",
            "training loss:  0.15, training accuracy:  96.91\n",
            "Epoch 3\n",
            "training loss:  0.14, training accuracy:  97.02\n",
            "Epoch 4\n",
            "training loss:  0.12, training accuracy:  97.18\n",
            "evaluation loss:  1.29, evaluation accuracy:  75.89\n"
          ]
        }
      ],
      "source": [
        "for i in range(combined_train_num_epochs):\n",
        "    print(\"Epoch\", i)\n",
        "    train_model(supervised_model, combined_dataloader, criterion, optimizer, device)\n",
        "\n",
        "evaluate_model(supervised_model, test_dataloader, criterion, device)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uT5ZI_sd9Q2y"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}